<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://sscast.github.io//feed.xml" rel="self" type="application/atom+xml" /><link href="https://sscast.github.io//" rel="alternate" type="text/html" /><updated>2019-12-06T16:01:52-05:00</updated><id>https://sscast.github.io//feed.xml</id><title type="html">Blog</title><subtitle>A collection of some interesting projects</subtitle><entry><title type="html">My Story With Language</title><link href="https://sscast.github.io//My-Story-with-Language/" rel="alternate" type="text/html" title="My Story With Language" /><published>2019-12-05T00:00:00-05:00</published><updated>2019-12-05T00:00:00-05:00</updated><id>https://sscast.github.io//My-Story-with-Language</id><content type="html" xml:base="https://sscast.github.io//My-Story-with-Language/">&lt;p&gt;My life has had a close relationship with many languages, and I have benefited from knowing multiple languages.&lt;/p&gt;

&lt;h1 id=&quot;learn-english-and-computer-languages&quot;&gt;learn english and computer languages&lt;/h1&gt;
&lt;p&gt;I was born speaking only Mandarin, and I started to learn English in secondary school, at age of 11, at my time when it was a mandatory class in school. I picked up Cantonese when I worked in Hong Kong (in my mid 20s) after my degrees. I studied computer science in college, where I learnt different kinds of programming languages. Object Oriented Programming started to gain its popularity when I was in college, so I can embark on OOP and OOD philosophies and made my journey now easier.&lt;/p&gt;

&lt;h1 id=&quot;impove-english-and-mandarin&quot;&gt;impove english and mandarin&lt;/h1&gt;
&lt;p&gt;I did translation and tutoring (Chinese) part time jobs when I was in college and between my degrees to make some pocket money. It turns out to become a process for me to improve my multilingual skills. After college, when I started to look for jobs, one of the main reasons I got my first job was because I speak mandarin, because a big clientele was from mainland China. After I moved to Hong Kong, the connection with mainland China became stronger, in other words, high profile clients and big deals, so being multilingual was more important to me. Completely separate English and Chinese was a big challenge I had to solve when I had to face clients, because they don’t speak English at all. I can’t use English words when I speak mandarin. Sounds familiar?&lt;/p&gt;

&lt;h1 id=&quot;learn-cantonese-and-improve-over-years&quot;&gt;learn cantonese and improve over years&lt;/h1&gt;
&lt;p&gt;I was a fan of Cantonese, so I took the advantage of being physically in Hong Kong and learnt how to speak Cantonese. By the time I left Hong Kong, I was very comfortable conversing in Cantonese, but I had some difficutlties of writing/typing it. To me, it’s hard to find the right word corresponding to Cantonese when it comes to text. However, when I worked at one of the big names in Silicon Valley, having to look at huge of amount of Cantonese sentences every day, suddenly, I realized I can type and write Cantonese without a problem. Again, one of the reasons I got this job was because I speak Mandarin and I know computer language, and one of the reasons I got promoted afterwards was because I know Cantonese, so I was paid to pick up some technologies because of my language skills. Why not?&lt;/p&gt;

&lt;h1 id=&quot;short-summary&quot;&gt;short summary&lt;/h1&gt;
&lt;p&gt;Throughout these years, I didn’t take classes to learn these languages (other than mandatory classes in school). The only language I took classes to learn was French but I gave up after the beginner’s level. The fact that I didn’t use it after class gave me a lot of pain to attend class too. So, my experiences proved the powerfulness of life long learning and on the job learning.&lt;/p&gt;

&lt;h1 id=&quot;driven-force&quot;&gt;driven force&lt;/h1&gt;
&lt;p&gt;What I have achieved is probably quite amazing to some people, but when I went through my life, the driven force was not big ambitions or egos, but simple wishes that I wanted to know more about it or I wanted my life to become easier so I can communicate with others better. Meanwhile, I kept an open mind when it comes to new opportunities and not to set boundaries to myself as to what I can do. Being able to work with smart people and interact with high profile clients provided me great opportunities to learn, and I’ve seen a lot others who have done better and mastered more languages. I decided to share my story here in the hope that it might become inspiration to someone.&lt;/p&gt;

&lt;h1 id=&quot;whats-next&quot;&gt;what’s next?&lt;/h1&gt;
&lt;p&gt;Looking back, I personally have benefited from the language skills and the more I know about a language, the more I realize that there is an amazing culture behind it. With globalization and technology advancement, we cannot stop the fact some less popular language will get even less popular. How we use technology to “preserve” it is something I’m very fascinated about. For example, one day in the future, you want to to know how to say something in one ancient language or dialect, rather than digging into some history and linguistic books(and probably given up before you find an answer), if you can simply ask “somebody”, then you can get your answer immediately. This sounds really great and make me excited! With fascination like that, I started to answer questions on Stack Exchange about Chinese language related questions. And I have also tried to know more about technologies that I can use to help me achieve this. I’m glad to make baby steps everyday. Again, I don’t want to set boundaries to what I can do in the future with what I’m doing right now, and I’m open to new opportunities that I can contribute and learn.&lt;/p&gt;</content><author><name>Sylvia</name></author><summary type="html">My life has had a close relationship with many languages, and I have benefited from knowing multiple languages.</summary></entry><entry><title type="html">A binary classifier on sentiment analysis</title><link href="https://sscast.github.io//UCI-Sentiment-labeled-data/" rel="alternate" type="text/html" title="A binary classifier on sentiment analysis" /><published>2019-11-08T21:14:38-05:00</published><updated>2019-11-08T21:14:38-05:00</updated><id>https://sscast.github.io//UCI-Sentiment-labeled-data</id><content type="html" xml:base="https://sscast.github.io//UCI-Sentiment-labeled-data/">&lt;h1 id=&quot;purpose&quot;&gt;Purpose&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;This exercise looks at some sentiment data, so as to experiment binary classification on text data and evaluate some ML models.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;the-dataset&quot;&gt;The dataset&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;This dataset contains 3000 reviews with scores&lt;/li&gt;
  &lt;li&gt;Score is either 1 (for positive) or 0 (for negative)&lt;/li&gt;
  &lt;li&gt;The sentences come from three different websites/fields:
    &lt;ol&gt;
      &lt;li&gt;imdb.com&lt;/li&gt;
      &lt;li&gt;amazon.com&lt;/li&gt;
      &lt;li&gt;yelp.com&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;For each website, there exists 1000 reviews, 500 positive and 500 negative. Those were selected randomly from larger datasets of reviews.
The reviews were selected to have a clearly positive or negative connotaton, with a goal that no neutral sentences to be selected.
You may download the data at this &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences&quot;&gt;link&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;considerations&quot;&gt;Considerations&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;The dataset has been selected with umambiguous label so it’s perfect to do model training; however, the data size is not big so data might not be generalized enough and thus, may not be a good dataset for deep learning models. However, it still worth to use it as an experiment to test our understandings about different models&lt;/li&gt;
  &lt;li&gt;The dataset is relatively clean: no need to deal with outliers, missing or misleading data, and analysis can be focused on model selection and classification&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;data-preprocessing&quot;&gt;Data Preprocessing&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Inspect data&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A quick glance of every dataset made me realize that there is something wrong with loading the data for imdb dataset:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../assets/images/shape.png&quot; alt=&quot;shape&quot; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The shape for imdb data doesn’t seem to be right, as we expect 1000 rows but there are only 748 rows.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Reload data&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I spent some time to reload imdb data without using pandas read_csv. After fixing the loading, adding a couple more columns including word count and data source, we can plot the data and have some idea&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Visualize&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../../../../assets/images/wordcount.png&quot; alt=&quot;word counts visulaization&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h1 id=&quot;the-data-suggested-to-me&quot;&gt;The data suggested to me&lt;/h1&gt;
  &lt;ul&gt;
    &lt;li&gt;imdb reviews are generally having more words than Amazon and Yelp reviews&lt;/li&gt;
    &lt;li&gt;Word count distribution of positive and negative reviews are both following normal distribution, sckewed towards less words, which makes sense, as not many people like to write very long reviews&lt;/li&gt;
    &lt;li&gt;both Amazon and imdb have relatively similar range of word counts for positive and negative reviews, however, Yelp tends to have noticeable longer negative reviews than positive ones. I will be curious to see the total number of positive vs negative reviews, and see if there are more negative reviews on Yelp than positive ones.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;process-data&quot;&gt;Process data&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;I cleaned up the data by removing punctuations and stopwords and some extended stopwords such as ‘s’, ‘ve’, ‘m’, ‘wasn’, ‘didn’, ‘t’ etc. , those incomplete words resulted from removing of apostrophe and not containing any meaning.&lt;/li&gt;
  &lt;li&gt;I have also removed words like ‘product’, ‘movie’, ‘service’ that are high frequency words for both positive and negative reviews, which are neutral in nature&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;clustering&quot;&gt;Clustering&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Let’s look at the word cloud plot to have a few of the top frequent words. The words shown in the white box are word cloud from Positive reviews and red box are from the negative reviews:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../../../../assets/images/positive.png&quot; alt=&quot;WordCloud&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;From a glance of the wordcloud, we can see some intereting findings, which will help our next steps for modeling. For example:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;“quality”, “sound” tend to be associated with positive reviews, and “phone”, “makes”, “bluetooth” tend to be associated with negative reviews. We can further manipulate the wordCloud based on the meaningfulness of the words, but currently it looks fine to me.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;classification&quot;&gt;Classification&lt;/h3&gt;

&lt;h4 id=&quot;logistic-regression&quot;&gt;Logistic Regression&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;The following are metrics I got after running logistic regression on the dataset, with a train/test split of 80/20:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;               precision    recall  f1-score   support

           0       0.80      0.84      0.82       299
           1       0.83      0.79      0.81       301

    accuracy                           0.81       600
   macro avg       0.81      0.81      0.81       600
weighted avg       0.81      0.81      0.81       600
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;The accuracy (81%) is not too bad, given that this is a small dataset, and I only did very simple data cleanning. Interesting to see that the precision is higher on positive reviews, and recall is higher on negative reviews. Given the size of the dataset, this might be simply based on randomness of the distributio of the train and test data.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I have tested the same model on a much larger dataset, with 50,000 training record, and I can easily achieve 90% accuracy, and higher precision and higher recall.&lt;/p&gt;

&lt;h4 id=&quot;naive-bayes&quot;&gt;Naive Bayes&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;               precision    recall  f1-score   support

           0       0.83      0.79      0.81       309
           1       0.79      0.83      0.81       291

    accuracy                           0.81       600
   macro avg       0.81      0.81      0.81       600
weighted avg       0.81      0.81      0.81       600
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;simple-mlp&quot;&gt;Simple MLP&lt;/h4&gt;

&lt;p&gt;This is a simple MLP model implemented with Keras and the code piece is borrowed from Keras API documentation.&lt;/p&gt;

&lt;p&gt;The result is similar to using Logistic Regression or Naive Bayes.&lt;/p&gt;

&lt;p&gt;Training accuracy and loss curve looks as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../assets/images/MLP_training.png&quot; alt=&quot;MLP_training&quot; /&gt;
&lt;img src=&quot;../../../../assets/images/MLP_loss.png&quot; alt=&quot;MLP_loss&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;models-comparison&quot;&gt;models comparison&lt;/h4&gt;

&lt;p&gt;ROC curve looks quite similar with the three models and we can also see the accuracy levels are similar too. imdb dataset seems fit better with the MLP model compared with the other two datasets.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../assets/images/roc_auc_curve.png&quot; alt=&quot;roc_auc_curve&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../assets/images/accuracy.png&quot; alt=&quot;accuracy&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;conclusion-and-next-steps&quot;&gt;Conclusion and next steps&lt;/h3&gt;
&lt;p&gt;The simple exercise shows how simple sentiment models work and work as a quick test of a dataset. What can be done further is to do a multiclassification classifier, with positive, negative and neutral reviews, as in real life, people do have neatral opinions.&lt;/p&gt;

&lt;p&gt;One thing to note is that the dataset for this model is hand crafted, by removing review score=3, and set review scores=1 and 2 as negative, and 4 and 5 as positive. This way has effectively deliminate some ambiguity, and thus making the model more accurate. However, the dataset I use with 50,000 record have only positive and negative labels too, but didn’t mention that it is prepared and selected based on the same methodology, also achieved pretty good accurarcy. So building a classifier should be very feasible and can be used in many real business use cases. With larger dataset, we can definitely choose a model to achieve better metrics. This is a good exercise to once again show that the problem for deep learning is not to achieve high accuracy, but to generalize.&lt;/p&gt;</content><author><name>Sylvia</name></author><summary type="html">Purpose</summary></entry></feed>